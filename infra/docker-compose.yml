services:
  postgres:
    image: postgres:16-alpine
    container_name: sports-postgres
    profiles: ["dev", "prod"]
    environment:
      POSTGRES_DB: ${POSTGRES_DB:-sports}
      POSTGRES_USER: ${POSTGRES_USER:-sports}
      POSTGRES_PASSWORD: ${POSTGRES_PASSWORD:-sports}
      PGDATA: /var/lib/postgresql/data/pgdata
    volumes:
      - postgres-data:/var/lib/postgresql/data
      - ./backups:/backups
      - ./scripts:/scripts
    ports:
      - "${POSTGRES_PORT:-5432}:5432"
    networks:
      - internal
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U ${POSTGRES_USER:-sports}"]
      interval: 10s
      timeout: 5s
      retries: 5

  redis:
    image: redis:7-alpine
    container_name: sports-redis
    profiles: ["dev", "prod"]
    command: >
      sh -c "redis-server --appendonly yes ${REDIS_PASSWORD:+--requirepass $REDIS_PASSWORD}"
    environment:
      REDIS_PASSWORD: ${REDIS_PASSWORD:-}
    networks:
      - internal
    healthcheck:
      test: ["CMD", "redis-cli", "${REDIS_PASSWORD:+-a}", "${REDIS_PASSWORD:-}", "ping"]
      interval: 10s
      timeout: 3s
      retries: 5

  api:
    image: ghcr.io/dock108dev/sports-data-admin-util-api:latest
    build:
      context: ..
      dockerfile: infra/api.Dockerfile
    container_name: sports-api
    profiles: ["dev", "prod"]
    environment:
      DATABASE_URL: postgresql+asyncpg://${POSTGRES_USER:-sports}:${POSTGRES_PASSWORD:-sports}@postgres:5432/${POSTGRES_DB:-sports}
      REDIS_URL: redis://${REDIS_PASSWORD:+:$REDIS_PASSWORD@}redis:6379/0
      # Celery broker uses database 2 to match scraper worker
      CELERY_BROKER_URL: redis://${REDIS_PASSWORD:+:$REDIS_PASSWORD@}redis:6379/2
      ALLOWED_CORS_ORIGINS: ${ALLOWED_CORS_ORIGINS:-}
      ENVIRONMENT: ${ENVIRONMENT:-development}
      RUN_MIGRATIONS: ${RUN_MIGRATIONS:-false}
      # OpenAI configuration for AI-enhanced timeline generation
      OPENAI_API_KEY: ${OPENAI_API_KEY:-}
      OPENAI_MODEL_CLASSIFICATION: ${OPENAI_MODEL_CLASSIFICATION:-gpt-4o-mini}
      OPENAI_MODEL_SUMMARY: ${OPENAI_MODEL_SUMMARY:-gpt-4o}
      ENABLE_AI_SOCIAL_ROLES: ${ENABLE_AI_SOCIAL_ROLES:-true}
      ENABLE_AI_SEGMENT_ENRICHMENT: ${ENABLE_AI_SEGMENT_ENRICHMENT:-true}
      ENABLE_AI_SUMMARY: ${ENABLE_AI_SUMMARY:-true}
    depends_on:
      postgres:
        condition: service_healthy
      redis:
        condition: service_healthy
    networks:
      - internal
      - app
    ports:
      - "8000:8000"
    healthcheck:
      test: ["CMD-SHELL", "python -c \"import urllib.request; urllib.request.urlopen('http://localhost:8000/healthz')\""]
      interval: 10s
      timeout: 5s
      retries: 5

  api-worker:
    image: ghcr.io/dock108dev/sports-data-admin-util-api:latest
    build:
      context: ..
      dockerfile: infra/api.Dockerfile
    container_name: sports-api-worker
    profiles: ["dev", "prod"]
    command: ["celery", "-A", "app.celery_app.celery_app", "worker", "--loglevel=info", "--concurrency=2"]
    environment:
      DATABASE_URL: postgresql+asyncpg://${POSTGRES_USER:-sports}:${POSTGRES_PASSWORD:-sports}@postgres:5432/${POSTGRES_DB:-sports}
      REDIS_URL: redis://${REDIS_PASSWORD:+:$REDIS_PASSWORD@}redis:6379/0
      # Must match API's CELERY_BROKER_URL so worker receives tasks
      CELERY_BROKER_URL: redis://${REDIS_PASSWORD:+:$REDIS_PASSWORD@}redis:6379/2
      ALLOWED_CORS_ORIGINS: ${ALLOWED_CORS_ORIGINS:-}
      ENVIRONMENT: ${ENVIRONMENT:-development}
      OPENAI_API_KEY: ${OPENAI_API_KEY:-}
      OPENAI_MODEL_CLASSIFICATION: ${OPENAI_MODEL_CLASSIFICATION:-gpt-4o-mini}
      OPENAI_MODEL_SUMMARY: ${OPENAI_MODEL_SUMMARY:-gpt-4o}
      ENABLE_AI_SOCIAL_ROLES: ${ENABLE_AI_SOCIAL_ROLES:-true}
      ENABLE_AI_SEGMENT_ENRICHMENT: ${ENABLE_AI_SEGMENT_ENRICHMENT:-true}
      ENABLE_AI_SUMMARY: ${ENABLE_AI_SUMMARY:-true}
    depends_on:
      postgres:
        condition: service_healthy
      redis:
        condition: service_healthy
    networks:
      - internal
      - app
    healthcheck:
      test: ["CMD-SHELL", "celery -A app.celery_app.celery_app inspect ping -d celery@$(hostname) --timeout=5 | grep -q pong"]
      interval: 30s
      timeout: 10s
      retries: 5

  scraper:
    image: ghcr.io/dock108dev/sports-data-admin-util-scraper:latest
    build:
      context: ..
      dockerfile: infra/scraper.Dockerfile
    container_name: sports-scraper
    profiles: ["dev", "prod"]
    environment:
      DATABASE_URL: postgresql+psycopg://${POSTGRES_USER:-sports}:${POSTGRES_PASSWORD:-sports}@postgres:5432/${POSTGRES_DB:-sports}
      REDIS_URL: redis://${REDIS_PASSWORD:+:$REDIS_PASSWORD@}redis:6379/2
      REDIS_HOST: redis
      REDIS_PASSWORD: ${REDIS_PASSWORD:-}
      REDIS_DB: "2"
      ODDS_API_KEY: ${ODDS_API_KEY:-}
      CBB_STATS_API_KEY: ${CBB_STATS_API_KEY:-}
      ENVIRONMENT: ${ENVIRONMENT:-development}
      X_AUTH_TOKEN: ${X_AUTH_TOKEN:-}
      X_CT0: ${X_CT0:-}
    depends_on:
      postgres:
        condition: service_healthy
      redis:
        condition: service_healthy
    networks:
      - internal
      - app
    volumes:
      - scraper-cache:/app/scraper/game_data
    healthcheck:
      test: ["CMD-SHELL", "celery -A sports_scraper.celery_app.app inspect ping -d celery@$(hostname) --timeout=5 | grep -q pong"]
      interval: 30s
      timeout: 10s
      retries: 5

  scraper-beat:
    image: ghcr.io/dock108dev/sports-data-admin-util-scraper:latest
    build:
      context: ..
      dockerfile: infra/scraper.Dockerfile
    container_name: sports-scraper-beat
    profiles: ["dev", "prod"]
    command: ["celery", "-A", "sports_scraper.celery_app.app", "beat", "--loglevel=info"]
    environment:
      DATABASE_URL: postgresql+psycopg://${POSTGRES_USER:-sports}:${POSTGRES_PASSWORD:-sports}@postgres:5432/${POSTGRES_DB:-sports}
      REDIS_URL: redis://${REDIS_PASSWORD:+:$REDIS_PASSWORD@}redis:6379/2
      REDIS_HOST: redis
      REDIS_PASSWORD: ${REDIS_PASSWORD:-}
      REDIS_DB: "2"
      ODDS_API_KEY: ${ODDS_API_KEY:-}
      CBB_STATS_API_KEY: ${CBB_STATS_API_KEY:-}
      ENVIRONMENT: ${ENVIRONMENT:-development}
      X_AUTH_TOKEN: ${X_AUTH_TOKEN:-}
      X_CT0: ${X_CT0:-}
    depends_on:
      postgres:
        condition: service_healthy
      redis:
        condition: service_healthy
    networks:
      - internal
      - app

  migrate:
    image: ghcr.io/dock108dev/sports-data-admin-util-api:latest
    build:
      context: ..
      dockerfile: infra/api.Dockerfile
    container_name: sports-migrate
    profiles: ["dev", "prod"]
    command: ["alembic", "-c", "/app/alembic.ini", "upgrade", "head"]
    environment:
      DATABASE_URL: postgresql+asyncpg://${POSTGRES_USER:-sports}:${POSTGRES_PASSWORD:-sports}@postgres:5432/${POSTGRES_DB:-sports}
      ENVIRONMENT: ${ENVIRONMENT:-development}
      CBB_STATS_API_KEY: ${CBB_STATS_API_KEY:-}
    depends_on:
      postgres:
        condition: service_healthy
    networks:
      - internal
      - app  # Needed for external API calls during data migrations

  web:
    image: ghcr.io/dock108dev/sports-data-admin-util-web:latest
    build:
      context: ..
      dockerfile: infra/web.Dockerfile
      args:
        NEXT_PUBLIC_SPORTS_API_URL: ${NEXT_PUBLIC_SPORTS_API_URL:-http://localhost:8000}
        SPORTS_API_INTERNAL_URL: ${SPORTS_API_INTERNAL_URL:-http://api:8000}
    container_name: sports-web
    profiles: ["dev", "prod"]
    environment:
      # Browser requests should hit the host-mapped API port.
      NEXT_PUBLIC_SPORTS_API_URL: ${NEXT_PUBLIC_SPORTS_API_URL:-http://localhost:8000}
      # Server-side Next.js fetches run inside the web container; use the internal service name.
      SPORTS_API_INTERNAL_URL: ${SPORTS_API_INTERNAL_URL:-http://api:8000}
      # Basic auth for production (uses same password as postgres)
      ADMIN_PASSWORD: ${POSTGRES_PASSWORD:-}
      ENVIRONMENT: ${ENVIRONMENT:-development}
    depends_on:
      - api
    networks:
      - app
    ports:
      - "3000:3000"
    healthcheck:
      # Healthcheck: verify main application is responsive
      test: ["CMD-SHELL", "curl -fsS http://localhost:3000/"]
      interval: 10s
      timeout: 5s
      retries: 5

  backup:
    image: postgres:16-alpine
    container_name: sports-backup
    profiles: ["dev", "prod"]
    depends_on:
      postgres:
        condition: service_healthy
    volumes:
      - ./backups:/backups
      - ./scripts:/scripts
    environment:
      POSTGRES_USER: ${POSTGRES_USER:-sports}
      POSTGRES_PASSWORD: ${POSTGRES_PASSWORD:-sports}
      POSTGRES_DB: ${POSTGRES_DB:-sports}
      PGPASSWORD: ${POSTGRES_PASSWORD:-sports}
      PGHOST: postgres
    networks:
      - internal
    entrypoint: /bin/sh
    command:
      - -c
      - |
        # Ensure we're using UTC for all time calculations
        export TZ=UTC
        echo "Backup service started (TZ=UTC). Running initial backup..."
        /scripts/backup.sh
        echo "Daily backup scheduled at 14:00 UTC (9 AM EST / 10 AM EDT)..."
        while true; do
          # Calculate seconds until next 14:00 UTC
          CURRENT_HOUR=$$(date +%H)
          CURRENT_MIN=$$(date +%M)
          CURRENT_SEC=$$(date +%S)
          # Seconds since midnight
          NOW_SECS=$$((CURRENT_HOUR * 3600 + CURRENT_MIN * 60 + CURRENT_SEC))
          # Target: 14:00 = 50400 seconds since midnight
          TARGET_SECS=50400
          if [ "$$NOW_SECS" -ge "$$TARGET_SECS" ]; then
            # Already past 14:00, wait until tomorrow
            SLEEP_SECS=$$((86400 - NOW_SECS + TARGET_SECS))
          else
            SLEEP_SECS=$$((TARGET_SECS - NOW_SECS))
          fi
          echo "Next backup in $$((SLEEP_SECS / 3600))h $$((SLEEP_SECS % 3600 / 60))m at 14:00 UTC"
          sleep "$$SLEEP_SECS"
          /scripts/backup.sh
        done

volumes:
  postgres-data:
  scraper-cache:

networks:
  internal:
    internal: true
  app:
